{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10359a37",
   "metadata": {},
   "source": [
    "## 03a - Assess Unique ILLs \n",
    "go through and get the number of outlinks between versions and compares their shared ILLs\n",
    "\n",
    "import:\n",
    "\n",
    "None\n",
    "    \n",
    "output:\n",
    "\n",
    "    03a_onlyEnNoILL.txt - link English Wikipedia, doesn't have an Arabic ILL\n",
    "    03a_enOutarILL.txt - in the english article and has an arabic ILL\n",
    "    03a_onlyEn.txt - outlink only in the English article,  where the Arabic ILL is in the outlinks, this is arabic\n",
    "    03a_onlyEn_english.txt - outlink only in the English article, where the Arabic ILL is in the outlinks, this is English\n",
    "\n",
    "    03a_onlyAr.txt - in Arabic article where the English ILL is in the outlinks, this is English\n",
    "    03a_onlyAr_arabic.txt - in Arabic article where the English ILL is in the outlinks, this is Arabic\n",
    "    03a_onlyArNoILL.txt - link in Arabic Wikipedia, doesn't have an English ILL\n",
    "    \n",
    "    \n",
    "    \n",
    "The Plan:\n",
    "- this goes through and does some initial work with the outlinks \n",
    "- it grabs all of the outlinks in the english and arabic versions of the arab spring\n",
    "- organize them by ones that are only in each language. \n",
    "- identify ones that are in both by if there were english outlinks\n",
    "- that had an arabic ILL and if that arabic ILL was also an outlink\n",
    "\n",
    "\n",
    "1. Get Outlinks\n",
    "2. Check ILL Symmetry - English\n",
    "3. Check ILL Symmetry - Arabic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1221d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wikifunctions as wf #Brian's stuff\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "import json, requests\n",
    "\n",
    "from urllib.parse import unquote\n",
    "\n",
    "\n",
    "#----\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf169a1",
   "metadata": {},
   "source": [
    "was having problems with wikifunctions so i just copied and pasted the code here and altered it for my code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5292f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_to_links(input,is_json=True):\n",
    "    # Initialize an empty list to store the links\n",
    "    outlinks_list = []\n",
    "    \n",
    "    if is_json:\n",
    "        page_html = input['parse']['text']#['*']\n",
    "    else:\n",
    "        page_html = input\n",
    "\n",
    "    # Parse the HTML into Beautiful Soup\n",
    "    soup = BeautifulSoup(page_html,'lxml')\n",
    "\n",
    "    # Remove sections at end\n",
    "    bad_sections = ['See_also','Notes','References','Bibliography','External_links']\n",
    "    bad_titles = ['Special:','Wikipedia:','Help:','Template:','Category:','International Standard','Portal:','s:','File:','Digital object identifier','(page does not exist)']\n",
    "    \n",
    "    sections = soup.find_all('h2')\n",
    "    for section in sections:\n",
    "#         print(section) #<h2 id=\"References\">References</h2>\n",
    "#         print(type(section)) #<class 'bs4.element.Tag'>\n",
    "#         if section.span['id'] in bad_sections:#<------------------------------THIS IS THE PROBLEM\n",
    "        if section['id'] in bad_sections:\n",
    "            # Clean out the divs\n",
    "            div_siblings = section.find_next_siblings('div')\n",
    "            for sibling in div_siblings:\n",
    "                sibling.clear()\n",
    "\n",
    "            # Clean out the ULs\n",
    "            ul_siblings = section.find_next_siblings('ul')\n",
    "            for sibling in ul_siblings:\n",
    "                sibling.clear()\n",
    "                \n",
    "    \n",
    "    # Delete tags associated with templates\n",
    "    for tag in soup.find_all('tr'):\n",
    "        tag.replace_with('')\n",
    "\n",
    "    # For each paragraph tag, extract the titles within the links\n",
    "    for para in soup.find_all('p'):\n",
    "        for link in para.find_all('a'):\n",
    "            if link.has_attr('title'):\n",
    "                title = link['title']\n",
    "                # Ignore links that aren't interesting or are redlinks\n",
    "                if all(bad not in title for bad in bad_titles) and 'redlink' not in link['href']:\n",
    "                    outlinks_list.append(title)\n",
    "\n",
    "    # For each unordered list, extract the titles within the child links\n",
    "    for unordered_list in soup.find_all('ul'):\n",
    "        for item in unordered_list.find_all('li'):\n",
    "            for link in item.find_all('a'):\n",
    "                if link.has_attr('title'):\n",
    "                    title = link['title']\n",
    "                    # Ignore links that aren't interesting or are redlinks\n",
    "                    if all(bad not in title for bad in bad_titles) and 'redlink' not in link['href']:\n",
    "                        outlinks_list.append(title)\n",
    "    \n",
    "    return outlinks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cff087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_outlinks(page_title, endpoint='en.wikipedia.org/w/api.php', redirects=1):\n",
    "    \"\"\"Takes a page title and returns a list of wiki-links on the page. The \n",
    "    list may contain duplicates and the position in the list is approximately \n",
    "    where the links occurred.\n",
    "    \n",
    "    page_title - a string with the title of the page on Wikipedia\n",
    "    endpoint - a string that points to the web address of the API.\n",
    "        This defaults to the English Wikipedia endpoint: 'en.wikipedia.org/w/api.php'\n",
    "        Changing the two letter language code will return a different language edition\n",
    "        The Wikia endpoints are slightly different, e.g. 'starwars.wikia.com/api.php'\n",
    "    redirects - 1 or 0 for whether to follow page redirects, defaults to 1\n",
    "    \n",
    "    Returns:\n",
    "    outlinks_per_lang - a dictionary keyed by language returning a dictionary \n",
    "        keyed by page title returning a list of outlinks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the response from the API for a query\n",
    "    # After passing a page title, the API returns the HTML markup of the current article version within a JSON payload\n",
    "    #req = requests.get('https://{2}.wikipedia.org/w/api.php?action=parse&format=json&page={0}&redirects={1}&prop=text&disableeditsection=1&disabletoc=1'.format(page_title,redirects,lang))\n",
    "    query_url = \"https://{0}\".format(endpoint)\n",
    "    query_params = {}\n",
    "    query_params['action'] = 'parse'\n",
    "    query_params['page'] = page_title\n",
    "    query_params['redirects'] = redirects\n",
    "    query_params['prop'] = 'text'\n",
    "    query_params['disableeditsection'] = 1\n",
    "    query_params['disabletoc'] = 1\n",
    "    query_params['format'] = 'json'\n",
    "    query_params['formatversion'] = 2\n",
    "    \n",
    "    json_response = requests.get(url = query_url, params = query_params).json()\n",
    "    \n",
    "    if 'parse' in json_response.keys():\n",
    "        links = parse_to_links(json_response)\n",
    "        final_title = json_response['parse']['title']\n",
    "    else:\n",
    "        links = list()\n",
    "        final_title = page_title\n",
    "        \n",
    "    return links "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0b03eb",
   "metadata": {},
   "source": [
    "## 1. Get Outlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6482e421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n"
     ]
    }
   ],
   "source": [
    "#English\n",
    "out_en_lyst = get_page_outlinks('Arab Spring')\n",
    "print(len(out_en_lyst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f0a9751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493\n"
     ]
    }
   ],
   "source": [
    "#Arabic\n",
    "out_ar_lyst = get_page_outlinks('الربيع العربي', endpoint='ar.wikipedia.org/w/api.php')\n",
    "print(len(out_ar_lyst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b459e1e",
   "metadata": {},
   "source": [
    "### 1a. Resolve Redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41f840cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517\n",
      "382\n"
     ]
    }
   ],
   "source": [
    "out_en_lyst_noReDir = wf.resolve_redirects(out_en_lyst)\n",
    "out_ar_lyst_noReDir = wf.resolve_redirects(out_ar_lyst,endpoint='ar.wikipedia.org/w/api.php')\n",
    "\n",
    "print(len(out_en_lyst_noReDir))\n",
    "print(len(out_ar_lyst_noReDir ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0348f",
   "metadata": {},
   "source": [
    "### 1b. Delete Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22121b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394\n"
     ]
    }
   ],
   "source": [
    "#get rid of duplicates in English\n",
    "out_en_lyst_clean = []\n",
    "\n",
    "for x in out_en_lyst_noReDir:\n",
    "    if x not in out_en_lyst_clean: \n",
    "        out_en_lyst_clean.append(x)\n",
    "\n",
    "\n",
    "\n",
    "#the number of unique English outlinks\n",
    "print(len(out_en_lyst_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c22a528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n"
     ]
    }
   ],
   "source": [
    "#get rid of duplicates Arabic\n",
    "out_ar_lyst_clean = []\n",
    "\n",
    "for x in out_ar_lyst_noReDir:\n",
    "    if x not in out_ar_lyst_clean: \n",
    "        out_ar_lyst_clean.append(x)\n",
    "\n",
    "\n",
    "#the number of unique Arabic outlinks\n",
    "print(len(out_ar_lyst_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3010d002",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b93f0872",
   "metadata": {},
   "source": [
    "## 2. Check ILL Symmetry - English\n",
    "\n",
    "go through ILLs of outlinks and see if the ILL is in the outlink of the other language (english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "892fe9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyEnNoILL = [] #a list of english links - this is an En ILL but no arabic ILL \n",
    "enOutarILL = [] #a list of english links - there is an arILL in out_ar_lyst_clean\n",
    "onlyEn = [] #a list of english links- that have an arabic out link but it is not in in out_ar_lyst_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829f0cb",
   "metadata": {},
   "source": [
    "go through english outlinks get the ones that have arabic ILLs and where there are none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d041c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all of the arabic ILL for english outlinks\n",
    "#if there is one it is added to:\n",
    "arILLforEnOutlinks = []\n",
    "\n",
    "#if there isn't one it is added to: \n",
    "onlyEnNoILL = [] \n",
    "\n",
    "for enOut in out_en_lyst_clean:\n",
    "    ILL = wf.get_interlanguage_links(enOut)\n",
    "    try:\n",
    "        arILL = ILL['ar']\n",
    "        arILLforEnOutlinks.append(arILL)\n",
    "        #print(arILL)\n",
    "    except KeyError as ke:\n",
    "        #print(enOut)\n",
    "        onlyEnNoILL.append(enOut) #no arabic ILL \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "369f5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks to see if the arabic ILLs are in the outlinks of the arabic page\n",
    "\n",
    "#if they are add it to: \n",
    "enOutarILL = []\n",
    "enOutarILL_arabic = []\n",
    "\n",
    "#if the arabic ILL is not referenced also in the arabic page add it to\n",
    "onlyEn = []\n",
    "onlyEn_english = []\n",
    "\n",
    "        \n",
    "for arOut in arILLforEnOutlinks: #goes through all of the arabic outlinks\n",
    "    ILL = wf.get_interlanguage_links(arOut, 'ar.wikipedia.org/w/api.php' )\n",
    "    #print(ILL)\n",
    "    enILL = ILL['en'] #goes backward to the english ILL\n",
    "        \n",
    "    if arOut in out_ar_lyst_clean: #there is arILL in the arabic outlinks \n",
    "        enOutarILL.append(enILL)\n",
    "        enOutarILL_arabic.append(arOut)\n",
    "        \n",
    "    else:\n",
    "        onlyEn.append(arOut) #this is if it has an arILl but it isn't in the outlinks\n",
    "        onlyEn_english.append(enILL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46f87640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "82\n",
      "82\n",
      "281\n"
     ]
    }
   ],
   "source": [
    "#Outlinks in English article that don't have an Arabic ILL\n",
    "print(len(onlyEnNoILL))\n",
    "\n",
    "#Outlinks in the English version of the article, and have an Arabic ILL in the Arabic version of the article\n",
    "print(len(enOutarILL))\n",
    "print(len(enOutarILL_arabic))\n",
    "\n",
    "#Outlinks that have and Arabic ILL but it isn't in the Arabic version of the Arab Spring article\n",
    "print(len(onlyEn)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8322a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_en_lyst_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e333c81",
   "metadata": {},
   "source": [
    "Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/03a_onlyEnNoILL.txt\", \"w\") as f:\n",
    "    for s in onlyEnNoILL:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ee184",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/03a_enOutarILL.txt\", \"w\") as f:\n",
    "    for s in enOutarILL:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1382041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/03a_onlyEn.txt\", \"w\") as f:\n",
    "    for s in onlyEn:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7008792",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/03a_onlyEn_english.txt\", \"w\") as f:\n",
    "    for s in onlyEn_english:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179675c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17bc9497",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dc02707",
   "metadata": {},
   "source": [
    "## 3. Check ILL Symmetry - Arabic\n",
    "\n",
    "go through arabic outlinks\n",
    "\n",
    "go through Arabic outlinks get the ones that have English ILLs and where there are none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25ef5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all of the english translations of the outlinks\n",
    "#if there is an english ILL\n",
    "enILLforArOutlinks = []\n",
    "#if there isn't one it is added to: \n",
    "onlyArNoILL = [] \n",
    "\n",
    "for arOut in out_ar_lyst_clean: #goes throguh all the arabic outlines\n",
    "    ILL = wf.get_interlanguage_links(arOut, endpoint='ar.wikipedia.org/w/api.php')\n",
    "    try:\n",
    "        enILL = ILL['en']\n",
    "        enILLforArOutlinks.append(enILL) #gets english ILL\n",
    "        #print(arILL)\n",
    "    except KeyError as ke:\n",
    "        onlyArNoILL.append(arOut) #no english ILL \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1361431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks to see if the engish ILLs are in the outlinks of the english page\n",
    "\n",
    "#if they are add it to: \n",
    "arOutenILL = []\n",
    "arOutenILL_english = []\n",
    "\n",
    "#if the english ILL is not referenced also in the arabic page add it to\n",
    "onlyAr = []\n",
    "onlyAr_arabic = []\n",
    "        \n",
    "for enOut in enILLforArOutlinks: #goes through all the ones with ILLs in english\n",
    "    ILL = wf.get_interlanguage_links(enOut, endpoint='ar.wikipedia.org/w/api.php')\n",
    "    arILL = ILL['ar'] #translates them back to arabic\n",
    "    #print(arILL)\n",
    "        \n",
    "    if enOut in out_en_lyst_clean: #there is enILL in the english outlinks \n",
    "        arOutenILL.append(arILL) #adds the arabic version\n",
    "        arOutenILL_english.append(enOut) #keept the english version\n",
    "        \n",
    "    else:\n",
    "        onlyAr.append(enOut) #this is if it has an arILl but it isn't in the outlinks\n",
    "        onlyAr_arabic.append(arILL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eab56062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "82\n",
      "82\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "#Outlinks that are only in the Arabic Wikipedia and have no English ILL\n",
    "print(len(onlyArNoILL))\n",
    "\n",
    "# Outlinks that have an English ILL nad that English ILL is in the English version of the article\n",
    "print(len(arOutenILL))\n",
    "print(len(arOutenILL_english))\n",
    "\n",
    "# OUtlinks that have and English ILL but it isn't mentioned in the English version of the article\n",
    "print(len(onlyAr)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9694b44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n"
     ]
    }
   ],
   "source": [
    "print(len(out_ar_lyst_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3a285",
   "metadata": {},
   "source": [
    "Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/03a_onlyAr.txt\", \"w\") as f:\n",
    "    for s in onlyAr:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/03a_onlyAr_arabic.txt\", \"w\") as f:\n",
    "    for s in onlyAr_arabic:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/03a_onlyArNoILL.txt\", \"w\") as f:\n",
    "    for s in onlyArNoILL:\n",
    "        f.write(str(s) +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9cee3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65819ad3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f054208d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f72bb852",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
